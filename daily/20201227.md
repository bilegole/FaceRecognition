今天考研结束，开始干活。
既然是工程设计，就首先需要确定要做什么。

首先，寻找一下人脸识别的综述把，去谷歌学术上找。

---

找到以下几篇：

1. 2003年的，
   -  DOI:https://doi.org/10.1145/954339.954342
   - 

2. 2018年的，
   - [10.1109/SIBGRAPI.2018.00067](https://doi.org/10.1109/SIBGRAPI.2018.00067)
   - 简述，一篇综述，内容很棒。
3. 2019年的，
   - https://doi.org/10.3390/s20020342
   - 简述：一篇综述，似乎是来自中东那一块的。

---

2018年的那篇综述，介绍了若干数据集，比如

> **CASIA WebFace**是一个数据集，包含约10万个主题的500K图像。它由CASIA小组自动收集[16]，然后手动进行精炼。正如通过查看名人或名人收集的集合所常见的那样，该集合在与对象相关的图像方面呈现出长尾巴分布[27]。这意味着大部分图像都包含一些频繁且通常更为著名的主题，而其他主题仅由少量图像描述。

> **VGGFace**是由牛津小组[18]提出的用于训练深度模型的数据集，该模型包含2,622个人的260万张面孔。与CASIA不同，该集合具有平坦的分布，即，每个主题由大约一千个样本描述，由于从Web引擎上抓取的图像，其中大部分是高质量的正面。而且，尽管努力清理了布景，但对象的类内差异也受到噪声（离群值）的影响，而CASIA WebFace包含的严重错误较少。

> **UMDFaces**是[19]发行的面孔。Bansal*等。* [19]通过Amazon Mechanical Turk（AMT）混合使用了人类注释器，并已经训练有素的基于深度的面部分析工具来构建中等大小的集合，例如比[16]，[18]更难的集合。。另一个特点UMDFaces是，与CASIA和VGGFace，该集合包含的事实*都*静止图像（通常高品质）和视频帧（通常受运动模糊）。该集合提供面部关键点，面部姿势角度，性别信息的注释。这些注解是使用[28]自动提取的。该集合包含用于8277个主题的静态图像中的367,888个面部注释，以及来自3,100个主题的大约22K视频的370万个带注释的视频帧。尽管UMDFaces数小于其他集合，如[19]所述，但它的姿态分布比CASIA和VGGFace宽。

> MS-Celeb-1M最初在多媒体社区中发布，然后在整个计算机视觉社区中传播[20]。它拥有约1000万名名人的图像1000万张。每个名人都有Bing搜索引擎使用名人的名字检索的100张图像，而检索结果中没有任何过滤。与MS-Celeb-1M的大小（1000万张图像）相比，所有先前的设置都显得苍白；但是，数据集的质量受到标签噪声，重复图像，集合中存在的非面部图像的严重影响，所有这些使得直接使用变得困难。毕竟，MS-Celeb-1M的发布是为了从嘈杂的标签中学习，并且从未被策划。

> **VGGFace2**是**VGGFace**的改进版本，旨在减轻其前身的不足。VGGFace2 [21]包含331万个图像，这些图像来自名人，教授或政客等名人，共收集了9,131个主题。与以前的图像相比，平均图像数量有所减少：平均而言，一个人被362.6媒体描述。尽管这个数字有所下降，但VGGFace2的设计可覆盖各种姿势，年龄和种族，并尽可能降低标签噪音。标签噪声的减少是通过手动和自动过程的相互作用实现的。

---

发现一篇论文，和数据集有关。

[Learning Face Representation from Scratch](http://www.cornell.edu/)

主要内容就是，通过爬虫爬取带有标签的图像，以此获取带有标记的数据。

同时以此构成数据集：CASIA-WebFace

失败，原数据集原生下载链接消失。不过网上有其他备份的副本。

---

开始处理VGG-Face数据集和论文。

